# Academic Research Agent ğŸ“šğŸ”¬

> ä¸€ä¸ª"å­¦æœ¯ä¸¥è°¨"ä¸"ç¤¾åŒºçƒ­åº¦"å…¼é¡¾çš„æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹

## ğŸ¯ æ ¸å¿ƒå®šä½

é’ˆå¯¹å„ä¸ªå­¦æœ¯é¢†åŸŸä¿¡æ¯çˆ†ç‚¸ç—›ç‚¹ï¼Œæ‰“é€ æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ã€‚å®ƒä¸ä»…èƒ½å›ç­”é—®é¢˜ï¼Œè¿˜èƒ½ä¸»åŠ¨æ•´ç†çŸ¥è¯†è„‰ç»œï¼Œå°†æ™¦æ¶©çš„è®ºæ–‡è½¬åŒ–ä¸ºå¤šæ¨¡æ€çš„æ˜“è¯»å†…å®¹ã€‚

## âœ¨ æ ¸å¿ƒåŠŸèƒ½

### 1. å…¨ç½‘æƒ…æŠ¥èšåˆ
ç”¨æˆ·è¾“å…¥ Topicï¼ˆå¦‚ "DeepSeek"ï¼‰ï¼ŒAgent è‡ªåŠ¨æ‹‰å–ï¼š
- **ArXiv**: æœ€æ–°å­¦æœ¯è®ºæ–‡
- **Semantic Scholar**: è®ºæ–‡å¼•ç”¨å…³ç³» + æ·±åº¦å­¦æœ¯æœç´¢
- **Hugging Face**: æ¨¡å‹/è®ºæ–‡/æ•°æ®é›†
- **Stack Overflow**: æŠ€æœ¯é—®ç­” + æœ€ä½³å®è·µ
- **Hacker News**: æå®¢ç¤¾åŒºçƒ­è®®
- **Social Media**: ç¤¾åŒºè®¨è®º/èˆ†æƒ…ï¼ˆTwitter/X, Reddit, GitHubç­‰ï¼‰

### 2. æ·±åº¦ RAG é—®ç­”
åŸºäºå¤šæºä¿¡æ¯ï¼Œæ”¯æŒç”¨æˆ·è‡ªç”±æé—®

### 3. å¤šæ¨¡æ€è¾“å‡º
- **Video Brief**: æ¨¡ä»¿ NotebookLLMï¼Œç”ŸæˆæŠ€æœ¯å†…å®¹çš„è§†é¢‘/éŸ³é¢‘è§£è¯»
- **Timeline Mapper**: è‡ªåŠ¨æ¢³ç†è¯¥æŠ€æœ¯çš„æ¼”è¿›æ—¶é—´è½´ï¼ŒçŸ¥è¯†æ ‘
- **One-Pager**: ç”Ÿæˆ"ä¸€é¡µçº¸"å¤‡å¿˜å½•ï¼ˆå«æ ¸å¿ƒç»“è®ºã€å‚æ•°ã€èµ„æºé“¾æ¥ï¼‰
- **è´¨é‡å®ˆæŠ¤**: è‡ªåŠ¨æ¸…æ´—æ— æ•ˆèµ„æºé“¾æ¥ï¼Œå¹¶è¡¥å…¨ Video Brief çš„ `duration_sec` / `visual_prompt`ï¼Œç¡®ä¿å¯ç›´æ¥ç”¨äºæ–‡ç”Ÿè§†é¢‘æµç¨‹

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

é¡¹ç›®é‡‡ç”¨åˆ†å±‚æ¶æ„è®¾è®¡ï¼Œå…± 5 ä¸ªé˜¶æ®µï¼š

| é˜¶æ®µ | å±‚çº§ | çŠ¶æ€ | æè¿° |
|------|------|------|------|
| Phase 1 | æ•°æ®å±‚ | âœ… å®Œæˆ | å¤šæºæ•°æ®æŠ“å– (ArXiv, HuggingFace, Social) |
| Phase 2 | çŸ¥è¯†å±‚ | âœ… å®Œæˆ | æ•°æ®å¤„ç† + å‘é‡å­˜å‚¨ |
| Phase 3 | æ™ºèƒ½å±‚ | âœ… å®Œæˆ | LLM + RAG + LangGraph Agent ç¼–æ’ |
| Phase 4 | è¾“å‡ºå±‚ | âœ… å®Œæˆ | Timeline / One-Pager / Video Brief + Slidev æˆç‰‡è§†é¢‘ï¼ˆå«æ—ç™½éŸ³è½¨ï¼‰ |
| Phase 5 | æ¥å£å±‚ | ğŸ”² å¾…å¼€å‘ | CLI, Web UI, API |

## ğŸ“ é¡¹ç›®ç»“æ„

```
AcademicResearchAgent/
â”œâ”€â”€ config/                      # ğŸ“¦ é…ç½®ç®¡ç†æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py              #    æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ settings.py              #    Pydantic é…ç½®ç±»å®šä¹‰
â”‚   â”œâ”€â”€ .env                     #    ç¯å¢ƒå˜é‡ (API Keysç­‰)
â”‚   â””â”€â”€ .env.example             #    ç¯å¢ƒå˜é‡ç¤ºä¾‹æ¨¡æ¿
â”‚
â”œâ”€â”€ models/                      # ğŸ“¦ æ•°æ®æ¨¡å‹æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py              #    æ¨¡å—å¯¼å‡º
â”‚   â””â”€â”€ schemas.py               #    Pydantic æ•°æ®æ¨¡å‹ (Paper, Model, SocialPostç­‰)
â”‚
â”œâ”€â”€ scrapers/                    # ğŸ“¦ æ•°æ®æŠ“å–æ¨¡å— (Phase 1)
â”‚   â”œâ”€â”€ __init__.py              #    æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ base.py                  #    BaseScraper æŠ½è±¡åŸºç±» + é™é€Ÿå™¨
â”‚   â”œâ”€â”€ arxiv_scraper.py         #    ArXiv è®ºæ–‡æŠ“å–å™¨
â”‚   â”œâ”€â”€ huggingface_scraper.py   #    HuggingFace æ¨¡å‹/æ•°æ®é›†æŠ“å–å™¨
â”‚   â”œâ”€â”€ semantic_scholar_scraper.py  # Semantic Scholar å­¦æœ¯æœç´¢
â”‚   â”œâ”€â”€ stackoverflow_scraper.py     # Stack Overflow æŠ€æœ¯é—®ç­”
â”‚   â”œâ”€â”€ hackernews_scraper.py        # Hacker News æå®¢ç¤¾åŒº
â”‚   â””â”€â”€ social/                  #    ç¤¾äº¤åª’ä½“å­æ¨¡å—
â”‚       â”œâ”€â”€ __init__.py          #        æ¨¡å—å¯¼å‡º
â”‚       â”œâ”€â”€ twitter_scraper.py   #        Twitter/X æŠ“å–å™¨
â”‚       â”œâ”€â”€ reddit_scraper.py    #        Reddit æŠ“å–å™¨
â”‚       â””â”€â”€ github_scraper.py    #        GitHub ä»“åº“æŠ“å–å™¨
â”‚
â”œâ”€â”€ aggregator/                  # ğŸ“¦ æ•°æ®èšåˆæ¨¡å— (Phase 1)
â”‚   â”œâ”€â”€ __init__.py              #    æ¨¡å—å¯¼å‡º
â”‚   â””â”€â”€ data_aggregator.py       #    å¤šæºå¹¶è¡Œèšåˆå™¨
â”‚
â”œâ”€â”€ processing/                  # ğŸ“¦ æ•°æ®å¤„ç†æ¨¡å— (Phase 2)
â”‚   â”œâ”€â”€ __init__.py              #    æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ cleaner.py               #    æ–‡æœ¬æ¸…æ´— (URL/HTML/ç‰¹æ®Šå­—ç¬¦å¤„ç†)
â”‚   â”œâ”€â”€ chunker.py               #    æ–‡æœ¬åˆ†å— (å›ºå®š/å¥å­/æ®µè½/é€’å½’ç­–ç•¥)
â”‚   â””â”€â”€ embedder.py              #    å‘é‡åŒ– (SiliconFlow/Jina/OpenAIç­‰)
â”‚
â”œâ”€â”€ storage/                     # ğŸ“¦ å­˜å‚¨æ¨¡å— (Phase 2)
â”‚   â”œâ”€â”€ __init__.py              #    æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ vector_store.py          #    å‘é‡æ•°æ®åº“ (Qdrant)
â”‚   â””â”€â”€ cache.py                 #    ç¼“å­˜ (å†…å­˜ç¼“å­˜/ç£ç›˜ç¼“å­˜)
â”‚
â”œâ”€â”€ intelligence/                # ğŸ“¦ æ™ºèƒ½æ¨¡å— (Phase 3)
â”‚   â”œâ”€â”€ __init__.py              #    æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ llm/                     #    LLM æŠ½è±¡å±‚
â”‚   â”‚   â”œâ”€â”€ base.py              #        BaseLLM æŠ½è±¡åŸºç±»
â”‚   â”‚   â”œâ”€â”€ openai_llm.py        #        OpenAI (GPT-4o)
â”‚   â”‚   â”œâ”€â”€ anthropic_llm.py     #        Anthropic (Claude)
â”‚   â”‚   â”œâ”€â”€ deepseek_llm.py      #        DeepSeek (V3/R1)
â”‚   â”‚   â”œâ”€â”€ gemini_llm.py        #        Google Gemini
â”‚   â”‚   â””â”€â”€ factory.py           #        get_llm() å·¥å‚å‡½æ•°
â”‚   â”œâ”€â”€ state/                   #    çŠ¶æ€å®šä¹‰
â”‚   â”‚   â””â”€â”€ agent_state.py       #        AgentState TypedDict
â”‚   â”œâ”€â”€ agents/                  #    å¤š Agent ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ search_agent.py      #        æœç´¢æƒ…æŠ¥å‘˜ (ReAct)
â”‚   â”‚   â”œâ”€â”€ analyst_agent.py     #        å­¦æœ¯åˆ†æå¸ˆ (RAG)
â”‚   â”‚   â””â”€â”€ content_agent.py     #        å†…å®¹å®˜ (å¹¶è¡Œç”Ÿæˆ)
â”‚   â”œâ”€â”€ tools/                   #    Agent å·¥å…·
â”‚   â”‚   â”œâ”€â”€ search_tools.py      #        æœç´¢å·¥å…·å°è£…
â”‚   â”‚   â””â”€â”€ rag_tools.py         #        RAG å·¥å…·å°è£…
â”‚   â””â”€â”€ graph/                   #    LangGraph ç¼–æ’
â”‚       â””â”€â”€ research_graph.py    #        ç ”ç©¶å·¥ä½œæµå›¾
â”‚
â”œâ”€â”€ outputs/                     # ğŸ“¦ è¾“å‡ºæ¨¡å— (Phase 4)
â”‚   â”œâ”€â”€ __init__.py              #    æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ models.py                #    è¾“å‡ºæ•°æ®ç»“æ„ (Timeline/OnePager/VideoBrief)
â”‚   â”œâ”€â”€ renderers.py             #    æ¸²æŸ“ (Markdown/Mermaid)
â”‚   â””â”€â”€ exporter.py              #    å¯¼å‡º (md/json/manifest)
â”‚
â”œâ”€â”€ utils/                       # ğŸ“¦ å·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py              #    æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ logger.py                #    Rich ç¾åŒ–æ—¥å¿—
â”‚   â””â”€â”€ exceptions.py            #    è‡ªå®šä¹‰å¼‚å¸¸ç±»å±‚æ¬¡
â”‚
â”œâ”€â”€ tests/                       # ğŸ“¦ æµ‹è¯•æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py              #    æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ test_scrapers.py         #    Phase 1 æŠ“å–å™¨æµ‹è¯•
â”‚   â””â”€â”€ test_processing_storage.py  # Phase 2 å¤„ç†å­˜å‚¨æµ‹è¯•
â”‚
â”œâ”€â”€ main.py                      # ğŸš€ Phase 1 ä¸»å…¥å£ (æ•°æ®æŠ“å–CLI)
â”œâ”€â”€ demo.py                      # ğŸ® Phase 1 æ¼”ç¤ºè„šæœ¬
â”œâ”€â”€ demo_phase2.py               # ğŸ® Phase 2 æ¼”ç¤ºè„šæœ¬
â”œâ”€â”€ demo_phase3.py               # ğŸ® Phase 3 æ¼”ç¤ºè„šæœ¬ (LLM + Agent)
â”œâ”€â”€ demo_phase4.py               # ğŸ® Phase 4 æ¼”ç¤ºè„šæœ¬ (Render + Export)
â”œâ”€â”€ requirements.txt             # ğŸ“‹ ä¾èµ–æ¸…å•
â””â”€â”€ README.md                    # ğŸ“– é¡¹ç›®æ–‡æ¡£
```

## ğŸ“ æ¨¡å—è¯¦è§£

### config/ - é…ç½®ç®¡ç†

| æ–‡ä»¶ | ç±»/å‡½æ•° | ä½œç”¨ |
|------|---------|------|
| `settings.py` | `Settings` | ä¸»é…ç½®ç±»ï¼Œèšåˆæ‰€æœ‰å­é…ç½® |
| | `EmbeddingSettings` | Embedding æœåŠ¡é…ç½® (provider, model_name, api_keys) |
| | `StorageSettings` | å­˜å‚¨é…ç½® (vector_db_path, cache_path) |
| | `LLMSettings` | LLM é…ç½® (provider, temperature, api_keys) |
| | `get_settings()` | è·å–å…¨å±€é…ç½®å•ä¾‹ |
| | `get_embedding_settings()` | è·å– Embedding é…ç½® |
| | `get_llm_settings()` | è·å– LLM é…ç½® |

### scrapers/ - æ•°æ®æŠ“å–

| æ–‡ä»¶ | ç±» | ä½œç”¨ |
|------|-----|------|
| `base.py` | `BaseScraper` | æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰ `search()` æ¥å£ |
| | `RateLimitedScraper` | å¸¦é™é€Ÿçš„æŠ½è±¡åŸºç±» |
| `arxiv_scraper.py` | `ArxivScraper` | ArXiv è®ºæ–‡æœç´¢ï¼Œè¿”å› `Paper` å¯¹è±¡ |
| `huggingface_scraper.py` | `HuggingFaceScraper` | HuggingFace æ¨¡å‹/æ•°æ®é›†æœç´¢ |
| `semantic_scholar_scraper.py` | `SemanticScholarScraper` | å­¦æœ¯è®ºæ–‡ + å¼•ç”¨å…³ç³» (1 req/s å…è´¹) |
| `stackoverflow_scraper.py` | `StackOverflowScraper` | æŠ€æœ¯é—®ç­” (æŒ‰æŠ•ç¥¨æ’åº) |
| `hackernews_scraper.py` | `HackerNewsScraper` | Algolia æœç´¢ + Firebase çƒ­é—¨ |
| `social/twitter_scraper.py` | `TwitterScraper` | Twitter/X æœç´¢ (éœ€ Bearer Token) |
| `social/reddit_scraper.py` | `RedditScraper` | Reddit å¸–å­æœç´¢ |
| `social/github_scraper.py` | `GitHubScraper` | GitHub ä»“åº“æœç´¢ |

### processing/ - æ•°æ®å¤„ç†

| æ–‡ä»¶ | ç±»/å‡½æ•° | ä½œç”¨ |
|------|---------|------|
| `cleaner.py` | `DataCleaner` | æ–‡æœ¬æ¸…æ´—å™¨ |
| | `clean_text()` | æ¸…æ´—å•æ®µæ–‡æœ¬ (å»URL/HTML/ç©ºç™½ç­‰) |
| | `clean_paper()` | æ¸…æ´—è®ºæ–‡æ•°æ® |
| `chunker.py` | `TextChunker` | æ–‡æœ¬åˆ†å—å™¨ |
| | `ChunkingStrategy` | åˆ†å—ç­–ç•¥æšä¸¾ (FIXED_SIZE/SENTENCE/PARAGRAPH/RECURSIVE) |
| | `DocumentChunk` | åˆ†å—ç»“æœæ•°æ®ç±» |
| | `chunk_text()` / `chunk_document()` | ä¾¿æ·å‡½æ•° |
| `embedder.py` | `SiliconFlowEmbedder` | SiliconFlow BGE-M3 (å…è´¹ï¼Œæ¨è) |
| | `JinaEmbedder` | Jina Embeddings API |
| | `OpenAIEmbedder` | OpenAI Embeddings API |
| | `SentenceTransformerEmbedder` | æœ¬åœ° SentenceTransformers |
| | `get_embedder()` | å·¥å‚å‡½æ•°ï¼Œè‡ªåŠ¨è¯»å– .env é…ç½® |

### storage/ - å­˜å‚¨

| æ–‡ä»¶ | ç±» | ä½œç”¨ |
|------|-----|------|
| `vector_store.py` | `QdrantVectorStore` | Qdrant å‘é‡å­˜å‚¨ (æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤ã€æŒä¹…åŒ–) |
| | `SearchResult` | æœç´¢ç»“æœæ•°æ®ç±» |
| | `get_vector_store()` | å·¥å‚å‡½æ•°ï¼Œè‡ªåŠ¨è¯»å– .env é…ç½® |
| `cache.py` | `MemoryCache` | å†…å­˜ç¼“å­˜ (å¸¦ TTL) |
| | `DiskCache` | ç£ç›˜ç¼“å­˜ (JSON æŒä¹…åŒ–) |

### intelligence/ - æ™ºèƒ½å±‚ (Phase 3)

| æ–‡ä»¶ | ç±»/å‡½æ•° | ä½œç”¨ |
|------|---------|------|
| `llm/base.py` | `BaseLLM` | LLM æŠ½è±¡åŸºç±» |
| | `Message` | å¯¹è¯æ¶ˆæ¯ç±» |
| | `LLMResponse` | LLM å“åº”ç±» |
| `llm/openai_llm.py` | `OpenAILLM` | OpenAI GPT-4o å®ç° |
| `llm/anthropic_llm.py` | `AnthropicLLM` | Claude 3.5 Sonnet å®ç° |
| `llm/deepseek_llm.py` | `DeepSeekLLM` | DeepSeek V3/R1 å®ç° |
| `llm/gemini_llm.py` | `GeminiLLM` | Gemini 2.0 å®ç° |
| `llm/factory.py` | `get_llm()` | å·¥å‚å‡½æ•°ï¼Œè‡ªåŠ¨è¯»å– .env é…ç½® |
| `agents/search_agent.py` | `SearchAgent` | æœç´¢æƒ…æŠ¥å‘˜ (ReAct é©±åŠ¨) |
| `agents/analyst_agent.py` | `AnalystAgent` | å­¦æœ¯åˆ†æå¸ˆ (RAG + äº‹å®éªŒè¯) |
| `agents/content_agent.py` | `ContentAgent` | å†…å®¹å®˜ (å¹¶è¡Œç”Ÿæˆ Timeline/One-Pager/Video) |
| `graph/research_graph.py` | `ResearchGraph` | LangGraph ç ”ç©¶å·¥ä½œæµ |
| | `create_research_graph()` | åˆ›å»ºå·¥ä½œæµå›¾ |
| | `run_research()` | æ‰§è¡Œç ”ç©¶ä»»åŠ¡ |

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# 1. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 2. é…ç½®ç¯å¢ƒå˜é‡
cp config/.env.example config/.env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å…¥ API Keys

# 3. è¿è¡Œ Phase 1 æ¼”ç¤º (æ•°æ®æŠ“å–)
python demo.py

# 4. è¿è¡Œ Phase 2 æ¼”ç¤º (å¤„ç†+å­˜å‚¨)
python demo_phase2.py

# 5. è¿è¡Œ Phase 3 æ¼”ç¤º (LLM + Agent)
python demo_phase3.py

# 6. è¿è¡Œ Phase 4 çœŸå®ç«¯åˆ°ç«¯ (å…³é”®è¯ -> æ·±åº¦è¾“å‡º)
python demo_phase4.py --topic "FlashAttention" -n 3 --sources "arxiv,huggingface,github"

# 7. å®‰è£… ffmpegï¼ˆSlidev è§†é¢‘ç¼–ç ä¾èµ–ï¼‰
# macOS:
brew install ffmpeg

# 8. å®‰è£…å…è´¹é«˜è´¨é‡ TTSï¼ˆæ¨èï¼‰
pip install edge-tts

# 9. è¿è¡Œ Phase 4 + è§†é¢‘äº§ç‰©ï¼ˆSlidev è®²è§£è§†é¢‘ + è‡ªåŠ¨æ—ç™½éŸ³è½¨ï¼‰
python demo_phase4.py --topic "FlashAttention" -n 3 --sources "arxiv,github" --generate-video --tts-provider auto --tts-speed 1.2 --narration-model deepseek-chat

# 10. Manus ç«¯åˆ°ç«¯æµ‹è¯•æ ·ä¾‹ï¼ˆå¤šæº + æ·±åº¦è¾“å‡º + æˆç‰‡è§†é¢‘ï¼‰
python demo_phase4.py --topic "manus" -n 5 --sources "arxiv,huggingface,github,semantic_scholar,stackoverflow,hackernews" --generate-video

# 11. å‘½ä»¤è¡Œæœç´¢ (Phase 1)
python main.py --topic "DeepSeek" -n 20

# 12. è¿è¡Œ Phase 5 Web UIï¼ˆä¸‰æ å¸ƒå±€ + SSE æ–‡æ¡£æµï¼‰
python demo_phase5.py --host 127.0.0.1 --port 8765
# æµè§ˆå™¨æ‰“å¼€ http://127.0.0.1:8765
```

### Phase 5 UI å¸ƒå±€è¯´æ˜

- å·¦æ ï¼ˆæ–‡æ¡£è¾“å‡ºï¼‰ï¼š`one_pager.md / timeline.md / report.md / video_brief.md` é€šè¿‡ SSE åˆ†å—æµå¼æ˜¾ç¤º
- ä¸­æ ï¼ˆAgent + Chatï¼‰ï¼šç ”ç©¶ä»»åŠ¡é…ç½®ã€æ‰§è¡Œæµæ—¥å¿—ã€åŸºäº KB çš„èŠå¤©é—®ç­”
- å³æ ï¼ˆè§†é¢‘è¾“å‡ºï¼‰ï¼šè§†é¢‘ç”Ÿæˆè¿›åº¦ã€è§†é¢‘æ’­æ”¾å™¨ã€äº§ç‰©å¿«æ·é“¾æ¥

## ğŸ”§ é…ç½®è¯´æ˜

æ‰€æœ‰é…ç½®é›†ä¸­åœ¨ `config/.env`ï¼Œä¿®æ”¹åè‡ªåŠ¨ç”Ÿæ•ˆï¼š

### Embedding é…ç½® (æ ¸å¿ƒ)

```env
# åˆ‡æ¢ Embedding ä¾›åº”å•†
EMBEDDING_PROVIDER=siliconflow   # å¯é€‰: siliconflow, jina, openai, sentence_transformers

# åˆ‡æ¢æ¨¡å‹ (å¯é€‰ï¼Œä¸å¡«ç”¨é»˜è®¤)
EMBEDDING_MODEL_NAME=BAAI/bge-m3

# API Keys (æŒ‰éœ€å¡«å†™)
SILICONFLOW_API_KEY=your_key    # å…è´¹æ³¨å†Œ: https://siliconflow.cn/
JINA_API_KEY=your_key           # å…è´¹é¢åº¦: https://jina.ai/
OPENAI_API_KEY=your_key         # ä»˜è´¹
```

**æ”¯æŒçš„ Embedding ä¾›åº”å•†ï¼š**

| Provider | ç¯å¢ƒå˜é‡ | é»˜è®¤æ¨¡å‹ | ç»´åº¦ | è´¹ç”¨ |
|----------|----------|----------|------|------|
| `siliconflow` | `SILICONFLOW_API_KEY` | BAAI/bge-m3 | 1024 | **å…è´¹** |
| `jina` | `JINA_API_KEY` | jina-embeddings-v3 | 1024 | å…è´¹é¢åº¦ |
| `openai` | `OPENAI_API_KEY` | text-embedding-3-small | 1536 | ä»˜è´¹ |
| `sentence_transformers` | æ— éœ€ | all-MiniLM-L6-v2 | 384 | æœ¬åœ°è¿è¡Œ |

### æ•°æ®æºé…ç½®

```env
# ArXiv (æ— éœ€ API Key)
ARXIV_MAX_RESULTS=50

# Hugging Face
HUGGINGFACE_TOKEN=hf_xxx

# Twitter/X (éœ€ç”³è¯·å¼€å‘è€…è´¦å·)
TWITTER_BEARER_TOKEN=xxx

# Reddit
REDDIT_CLIENT_ID=xxx
REDDIT_CLIENT_SECRET=xxx

# GitHub
GITHUB_TOKEN=xxx
```

### å­˜å‚¨é…ç½® (å‘é‡æ•°æ®åº“)

```env
# å‘é‡æ•°æ®åº“é€‰æ‹©
STORAGE_VECTOR_DB_PROVIDER=qdrant
STORAGE_VECTOR_DB_PATH=./data/qdrant_db
STORAGE_CACHE_PATH=./data/cache
STORAGE_CACHE_TTL=3600   # ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)

# Qdrant Cloud (å¯é€‰ï¼Œç”¨äºè¿œç¨‹éƒ¨ç½²)
# STORAGE_QDRANT_URL=https://xxx.qdrant.io
# STORAGE_QDRANT_API_KEY=your_qdrant_api_key
```

**æ”¯æŒçš„å‘é‡æ•°æ®åº“ï¼š**

| Provider | ç‰¹æ€§ | é€‚ç”¨åœºæ™¯ |
|----------|------|----------|
| `qdrant` | å…ƒæ•°æ®è¿‡æ»¤ã€æŒä¹…åŒ–ã€å¹¶å‘å®‰å…¨ã€å¯å†…åµŒè¿è¡Œ | ç”Ÿäº§ç¯å¢ƒ |

**Qdrant å…ƒæ•°æ®è¿‡æ»¤ç¤ºä¾‹ï¼š**

```python
# æœç´¢ 2023 å¹´ä»¥åå…³äº LLM çš„è®ºæ–‡
results = store.search(
    "transformer architecture",
    filter={"year": {"$gte": 2023}, "topic": "LLM"}
)
```

### LLM é…ç½® (å¤šæœåŠ¡å•†æ”¯æŒ)

```env
# LLM æœåŠ¡å•†é€‰æ‹©: openai, anthropic, deepseek, gemini
LLM_PROVIDER=deepseek

# LLM é€šç”¨è®¾ç½®
LLM_MODEL_NAME=          # å¯é€‰ï¼Œä¸å¡«åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=4096

# OpenAI (GPT-4o)
LLM_OPENAI_API_KEY=sk-xxx

# Anthropic (Claude 3.5 Sonnet)
LLM_ANTHROPIC_API_KEY=sk-ant-xxx

# DeepSeek (V3/R1 - æ¨èï¼Œæ€§ä»·æ¯”é«˜)
LLM_DEEPSEEK_API_KEY=sk-xxx

# Google Gemini (Gemini 2.0)
LLM_GEMINI_API_KEY=AIza-xxx
```

### è§†é¢‘ç”Ÿæˆé…ç½® (Phase 4)

è¯´æ˜:
- å½“å‰ä»…ä¿ç•™ Slidev å•è·¯çº¿ï¼šå°† `video_brief + one_pager + facts` ç”Ÿæˆ Slidev å¹»ç¯ç‰‡ï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆ TTS æ—ç™½éŸ³è½¨ï¼Œæœ€ç»ˆä½¿ç”¨ ffmpeg åˆæˆ mp4ã€‚
- é»˜è®¤ä¸ç”Ÿæˆè§†é¢‘ï¼šåªæœ‰æ˜¾å¼ä¼  `--generate-video` æ‰ä¼šè¿›å…¥è§†é¢‘æµç¨‹ã€‚
- å³ä½¿è§†é¢‘ç”Ÿæˆå¤±è´¥ï¼Œæ–‡æ¡£äº§ç‰©ï¼ˆreport/one-pager/timeline/video-briefï¼‰ä»ä¼šå®Œæ•´å¯¼å‡ºã€‚
- å¯é€šè¿‡ `--slides-target-duration-sec` ä¸ `--slides-fps` æ§åˆ¶è®²è§£è§†é¢‘æ—¶é•¿å’Œå¸§ç‡ã€‚
- é»˜è®¤å¯ç”¨æ—ç™½ï¼šå¯ç”¨ `--disable-narration` è¾“å‡ºé™éŸ³è§†é¢‘ï¼›å¯ç”¨ `--tts-provider` / `--tts-voice` / `--tts-speed` æ§åˆ¶éŸ³è‰²ä¸è¯­é€Ÿã€‚
- æ¯é¡µæ—ç™½å…ˆç”±å°æ¨¡å‹é‡å†™ï¼ˆé»˜è®¤ `deepseek-chat`ï¼Œå¯ç”¨ `--narration-model` è°ƒæ•´ï¼‰ï¼Œé¿å…é€å­—å¿µ PPTã€‚
- ç¿»é¡µæ—¶é•¿ç”±æ¯é¡µçœŸå®éŸ³è½¨æ—¶é•¿å†³å®šï¼Œä¸å†æŒ‰å›ºå®šç§’æ•°ç¡¬åˆ‡é¡µï¼Œé¿å…é•¿æ—¶é—´é™é»˜ç©ºçª—ã€‚
- TTS provider è‡ªåŠ¨å›é€€é¡ºåºï¼š`edge-tts` -> `say`ï¼ˆmacOSï¼‰-> `espeak`ï¼ˆLinuxï¼‰ã€‚
- é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨åœ¨ `data/.slidev_runtime` å®‰è£… `@slidev/cli + @slidev/theme-default + playwright-chromium`ã€‚

æ¨èç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰ï¼š

```env
# Edge-TTS (å›é€€ä¼˜å…ˆçº§ 1)
# æ¨è voice: zh-CN-YunxiNeural / en-US-GuyNeural
# å¯ç”¨ --tts-voice è¦†ç›–

# æ—ç™½è„šæœ¬é‡å†™ï¼ˆå¯é€‰ï¼‰
LLM_DEEPSEEK_API_KEY=sk-xxx
VIDEO_NARRATION_DEEPSEEK_MODEL=deepseek-chat
```

**æ”¯æŒçš„ LLM æœåŠ¡å•†ï¼š**

| Provider | é»˜è®¤æ¨¡å‹ | ç‰¹æ€§ | æ¨èåœºæ™¯ |
|----------|----------|------|----------|
| `deepseek` | deepseek-chat | æ€§ä»·æ¯”é«˜ã€ä¸­æ–‡å‹å¥½ | ğŸŒŸ æ—¥å¸¸ç ”ç©¶ |
| `openai` | gpt-4o | èƒ½åŠ›å¼ºã€ç”Ÿæ€å®Œå–„ | å¤æ‚åˆ†æ |
| `anthropic` | claude-3-5-sonnet | é•¿ä¸Šä¸‹æ–‡ã€å®‰å…¨å¯¹é½ | é•¿æ–‡æ¡£å¤„ç† |
| `gemini` | gemini-2.0-flash | å¤šæ¨¡æ€ã€é€Ÿåº¦å¿« | å¿«é€ŸåŸå‹ |

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
from intelligence import get_llm, Message

# è‡ªåŠ¨è¯»å– .env é…ç½®
llm = get_llm()

# æˆ–æŒ‡å®šæœåŠ¡å•†
llm = get_llm(provider="anthropic")

# è°ƒç”¨ LLM
response = await llm.acomplete([
    Message(role="user", content="ä»€ä¹ˆæ˜¯ Transformer æ¶æ„?")
])
print(response.content)
```

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: æœç´¢å¹¶èšåˆå¤šæºæ•°æ®

```python
import asyncio
from aggregator import DataAggregator

async def main():
    aggregator = DataAggregator()
    result = await aggregator.aggregate("Transformer", max_results=20)
    
    print(f"è®ºæ–‡: {len(result.papers)}")
    print(f"æ¨¡å‹: {len(result.models)}")
    print(f"æ•°æ®é›†: {len(result.datasets)}")
    print(f"ç¤¾äº¤å¸–å­: {len(result.social_posts)}")

asyncio.run(main())
```

### ç¤ºä¾‹ 2: å¤„ç†å¹¶å‘é‡åŒ–æ–‡æ¡£

```python
from processing import clean_text, chunk_document, get_embedder
from storage import QdrantVectorStore

# 1. æ¸…æ´—
text = clean_text(raw_text, remove_urls=True)

# 2. åˆ†å—
chunks = chunk_document(text, doc_id="paper_001", chunk_size=200)

# 3. å‘é‡åŒ– (è‡ªåŠ¨ä» .env è¯»å–é…ç½®)
embedder = get_embedder()
embeddings = embedder.embed([c.content for c in chunks])

# 4. å­˜å‚¨ (ä½¿ç”¨ Qdrant)
store = QdrantVectorStore(collection_name="papers", dimension=embedder.dimension)
store.add_with_embeddings(
    documents=[c.content for c in chunks],
    embeddings=embeddings.tolist(),
    metadatas=[c.metadata for c in chunks],
    ids=[c.id for c in chunks],
)

# 5. æœç´¢ (æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤!)
results = store.search("attention mechanism", top_k=5)
for r in results:
    print(f"[{r.score:.3f}] {r.content[:100]}...")

# 6. å¸¦å…ƒæ•°æ®è¿‡æ»¤çš„æœç´¢ (Qdrant ç‰¹æ€§)
results = store.search(
    "LLM architectures",
    top_k=5,
    filter={"year": {"$gte": 2023}}  # åªæœç´¢2023å¹´ä»¥åçš„è®ºæ–‡
)
```

### ç¤ºä¾‹ 3: åˆ‡æ¢ Embedding æ¨¡å‹

åªéœ€ä¿®æ”¹ `config/.env`ï¼š

```env
# æ–¹å¼1: ä½¿ç”¨ SiliconFlow BGE-M3 (å…è´¹)
EMBEDDING_PROVIDER=siliconflow
SILICONFLOW_API_KEY=sk-xxx

# æ–¹å¼2: ä½¿ç”¨ Jina
EMBEDDING_PROVIDER=jina
JINA_API_KEY=jina_xxx

# æ–¹å¼3: ä½¿ç”¨æœ¬åœ°æ¨¡å‹ (æ— éœ€API)
EMBEDDING_PROVIDER=sentence_transformers
```

ä»£ç æ— éœ€ä¿®æ”¹ï¼Œ`get_embedder()` ä¼šè‡ªåŠ¨è¯»å–é…ç½®ã€‚

### ç¤ºä¾‹ 4: ä½¿ç”¨ LangGraph ç ”ç©¶æµç¨‹

```python
import asyncio
from intelligence import ResearchGraph
from outputs import export_research_outputs, render_one_pager_markdown

async def main():
    # åˆ›å»ºç ”ç©¶å›¾
    graph = ResearchGraph()
    
    # è¿è¡Œå®Œæ•´ç ”ç©¶æµç¨‹
    result = await graph.run("Transformer æ³¨æ„åŠ›æœºåˆ¶çš„æœ€æ–°è¿›å±•")
    
    # æ¸²æŸ“è¾“å‡ºï¼ˆPhase 4ï¼‰
    print(render_one_pager_markdown(result.get("one_pager"), default_title="One-Pager"))

    # å¯¼å‡ºåˆ°ç›®å½•ï¼ˆç”Ÿæˆ Markdown + JSON + manifestï¼‰
    export_research_outputs(
        "./data/outputs/transformer_demo",
        topic=result["topic"],
        timeline=result.get("timeline"),
        one_pager=result.get("one_pager"),
        video_brief=result.get("video_brief"),
    )

asyncio.run(main())
```

### ç¤ºä¾‹ 5: ä½¿ç”¨å•ç‹¬çš„ Agent

```python
import asyncio
from intelligence import SearchAgent, AnalystAgent, ContentAgent
from outputs import render_research_report_markdown

async def main():
    topic = "Mamba architecture"
    
    # 1. æœç´¢ Agent (ReAct æ¨¡å¼)
    search_agent = SearchAgent(max_iterations=3)
    search_results = await search_agent.search(topic)
    print(f"æ‰¾åˆ° {len(search_results)} æ¡ç»“æœ")
    
    # 2. åˆ†æ Agent (RAG æ¨¡å¼)
    analyst = AnalystAgent()
    analysis = await analyst.analyze(topic, search_results)
    facts = analysis.get("facts", [])
    print(f"æå– {len(facts)} æ¡äº‹å®")
    
    # 3. å†…å®¹ Agent (å¹¶è¡Œç”Ÿæˆ)
    content_agent = ContentAgent()
    outputs = await content_agent.generate(topic, facts)
    
    report_md = render_research_report_markdown(
        topic,
        timeline=outputs.get("timeline"),
        one_pager=outputs.get("one_pager"),
        video_brief=outputs.get("video_brief"),
    )
    print(report_md)

asyncio.run(main())
```

### ç¤ºä¾‹ 6: å…³é”®è¯é©±åŠ¨çš„ç«¯åˆ°ç«¯æµç¨‹ï¼ˆæ¨èï¼‰

```python
import asyncio
from intelligence import run_research_end_to_end

async def main():
    result = await run_research_end_to_end(
        topic="FlashAttention",
        max_results_per_source=3,
        show_progress=False,
        generate_video=True,
        enable_knowledge_indexing=False,
        aggregator_kwargs={
            "enable_arxiv": True,
            "enable_huggingface": True,
            "enable_github": True,
            "enable_twitter": False,
            "enable_reddit": False,
            "enable_semantic_scholar": False,
            "enable_stackoverflow": False,
            "enable_hackernews": False,
        },
    )

    print(result["output_dir"])
    print(result["depth_assessment"])
    print(result["video_artifact"])

asyncio.run(main())
```

## ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œ Phase 1 æµ‹è¯• (æ•°æ®æŠ“å–)
python -m pytest tests/test_scrapers.py -v

# è¿è¡Œ Phase 2 æµ‹è¯• (å¤„ç†+å­˜å‚¨)
python tests/test_processing_storage.py

# è¿è¡Œ Phase 2 æ¼”ç¤º
python demo_phase2.py

# è¿è¡Œ Phase 4 è¾“å‡ºä¸è§†é¢‘æµ‹è¯•
python -m pytest tests/test_outputs.py tests/test_video_generator.py tests/test_end_to_end_pipeline.py tests/test_github_scraper_unit.py -q

# ä»…è¿è¡Œ Manus ç«¯åˆ°ç«¯ç”¨ä¾‹
python -m pytest tests/test_end_to_end_pipeline.py -k manus -q
```

## ğŸ—ºï¸ å¼€å‘è·¯çº¿å›¾

- [x] **Phase 1**: æ•°æ®æŠ“å–å±‚ (ArXiv, HuggingFace, Twitter, Reddit, GitHub)
- [x] **Phase 2**: å¤„ç†å­˜å‚¨å±‚ (Cleaner, Chunker, Embedder, VectorStore, Cache)
- [x] **Phase 3**: æ™ºèƒ½å±‚ (LLM æŠ½è±¡, å¤š Agent åä½œ, LangGraph ç¼–æ’)
- [x] **Phase 4**: è¾“å‡ºå±‚ (Timeline, One-Pager, Video Brief, å¸¦æ—ç™½éŸ³è½¨è§†é¢‘)
- [ ] **Phase 5**: æ¥å£å±‚ (CLI å¢å¼º, Web UI, REST API)

## ğŸ“ License

MIT License

MIT License
