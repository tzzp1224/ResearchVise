"""
Tests for Processing and Storage modules
"""
import asyncio
import os
import pytest
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))


class TestCleaner:
    """æ•°æ®æ¸…æ´—æµ‹è¯•"""
    
    def test_clean_text_basic(self):
        from processing import clean_text
        
        text = "Hello  World!   Multiple   spaces"
        cleaned = clean_text(text)
        assert "  " not in cleaned
    
    def test_clean_text_urls(self):
        from processing import clean_text
        
        text = "Check this out https://example.com and more"
        cleaned = clean_text(text, remove_urls=True)
        assert "https://" not in cleaned
    
    def test_clean_text_html(self):
        from processing import clean_text
        
        text = "Hello &amp; World &lt;tag&gt;"
        cleaned = clean_text(text)
        assert "&amp;" not in cleaned
        assert "& World" in cleaned


class TestChunker:
    """æ–‡æœ¬åˆ†å—æµ‹è¯•"""
    
    def test_chunk_basic(self):
        from processing import chunk_text
        
        text = "This is a test. " * 100
        chunks = chunk_text(text, chunk_size=200)
        
        assert len(chunks) > 1
        for chunk in chunks:
            assert len(chunk.content) <= 250  # å…è®¸ä¸€äº›ä½™é‡
    
    def test_chunk_with_metadata(self):
        from processing import chunk_document
        
        content = "Abstract paragraph. " * 50
        chunks = chunk_document(
            content=content,
            doc_id="paper_001",
            doc_type="paper",
            metadata={"title": "Test Paper"},
        )
        
        assert len(chunks) > 0
        assert chunks[0].metadata["doc_type"] == "paper"
        assert chunks[0].metadata["title"] == "Test Paper"
    
    def test_chunk_strategies(self):
        from processing import TextChunker, ChunkingStrategy
        
        text = """
        First paragraph with some content.
        
        Second paragraph with more content.
        
        Third paragraph with even more content.
        """
        
        # æŒ‰æ®µè½åˆ†å—
        chunker = TextChunker(
            strategy=ChunkingStrategy.PARAGRAPH,
            chunk_size=100,
        )
        chunks = chunker.chunk(text)
        
        assert len(chunks) >= 1


class TestCache:
    """ç¼“å­˜æµ‹è¯•"""
    
    def test_memory_cache_basic(self):
        from storage import MemoryCache
        
        cache = MemoryCache()
        
        cache.set("key1", "value1")
        assert cache.get("key1") == "value1"
        
        cache.delete("key1")
        assert cache.get("key1") is None
    
    def test_memory_cache_ttl(self):
        from storage import MemoryCache
        import time
        
        cache = MemoryCache(ttl=1)  # 1ç§’è¿‡æœŸ
        
        cache.set("key1", "value1")
        assert cache.get("key1") == "value1"
        
        time.sleep(1.5)
        assert cache.get("key1") is None
    
    def test_disk_cache_basic(self):
        from storage import DiskCache
        import tempfile
        import shutil
        
        cache_dir = tempfile.mkdtemp()
        
        try:
            cache = DiskCache(cache_dir=cache_dir)
            
            cache.set("key1", {"data": [1, 2, 3]})
            result = cache.get("key1")
            
            assert result == {"data": [1, 2, 3]}
        finally:
            shutil.rmtree(cache_dir)


class TestVectorStore:
    """å‘é‡å­˜å‚¨æµ‹è¯•"""
    
    @pytest.mark.slow
    def test_qdrant_basic(self):
        from storage import QdrantVectorStore
        
        # ä½¿ç”¨å†…å­˜æ¨¡å¼
        store = QdrantVectorStore(
            collection_name="test_collection",
            persist_directory=None,  # å†…å­˜æ¨¡å¼
        )
        
        # æ·»åŠ æ–‡æ¡£
        docs = [
            "Machine learning is a subset of artificial intelligence.",
            "Deep learning uses neural networks with many layers.",
            "Natural language processing handles text and speech.",
        ]
        
        ids = store.add(docs)
        assert len(ids) == 3
        assert store.count() == 3
        
        # æœç´¢
        results = store.search("What is deep learning?", top_k=2)
        assert len(results) <= 2
        assert results[0].score > 0
        
        # æ¸…ç©º
        store.clear()
    
    @pytest.mark.slow
    def test_qdrant_with_metadata(self):
        from storage import QdrantVectorStore

        store = QdrantVectorStore(
        )
        
        docs = ["Doc about AI", "Doc about ML"]
        metadatas = [{"type": "ai"}, {"type": "ml"}]
        
        store.add(docs, metadatas)
        
        # å¸¦è¿‡æ»¤æ¡ä»¶æœç´¢
        results = store.search(
            "artificial intelligence",
            top_k=5,
            filter={"type": "ai"},
        )
        
        assert len(results) >= 1
        assert results[0].metadata.get("type") == "ai"
        
        store.clear()


class TestEmbedder:
    """Embedder æµ‹è¯•"""
    
    @pytest.mark.slow
    def test_sentence_transformer_embedder(self):
        from processing import SentenceTransformerEmbedder
        
        embedder = SentenceTransformerEmbedder(
            model_name="all-MiniLM-L6-v2"
        )
        
        # å•ä¸ªæ–‡æœ¬
        embedding = embedder.embed("Hello world")
        assert embedding.shape == (1, 384)
        
        # å¤šä¸ªæ–‡æœ¬
        embeddings = embedder.embed(["Hello", "World"])
        assert embeddings.shape == (2, 384)
    
    @pytest.mark.slow
    def test_embedder_factory(self):
        from processing import get_embedder
        
        embedder = get_embedder(provider="sentence_transformers")
        assert embedder.dimension > 0
    
    def test_siliconflow_embedder_without_key(self):
        """æµ‹è¯• SiliconFlow Embedder (æ—  API Key åº”æŠ›å‡ºå¼‚å¸¸)"""
        import os
        from processing import SiliconFlowEmbedder
        from utils.exceptions import EmbeddingError
        
        # ä¿å­˜åŸå€¼
        original_key = os.environ.get("SILICONFLOW_API_KEY")
        
        try:
            # æ¸…é™¤ç¯å¢ƒå˜é‡
            if "SILICONFLOW_API_KEY" in os.environ:
                del os.environ["SILICONFLOW_API_KEY"]
            
            embedder = SiliconFlowEmbedder(api_key=None)
            
            # è°ƒç”¨åº”æŠ›å‡ºå¼‚å¸¸
            try:
                embedder.embed("test")
                assert False, "Should have raised EmbeddingError"
            except EmbeddingError:
                pass  # é¢„æœŸè¡Œä¸º
        finally:
            # æ¢å¤åŸå€¼
            if original_key:
                os.environ["SILICONFLOW_API_KEY"] = original_key
    
    @pytest.mark.skipif(
        not os.environ.get("SILICONFLOW_API_KEY"),
        reason="éœ€è¦ SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡"
    )
    def test_siliconflow_embedder_with_key(self):
        """æµ‹è¯• SiliconFlow BGE-M3 Embedder (éœ€è¦ API Key)"""
        from processing import SiliconFlowEmbedder
        
        embedder = SiliconFlowEmbedder()
        
        # å•ä¸ªæ–‡æœ¬
        embedding = embedder.embed("æœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ ")
        assert embedding.shape == (1, 1024)
        
        # å¤šä¸ªæ–‡æœ¬
        embeddings = embedder.embed(["Hello", "World", "ä½ å¥½"])
        assert embeddings.shape == (3, 1024)


# é›†æˆæµ‹è¯•
class TestIntegration:
    """é›†æˆæµ‹è¯•"""
    
    @pytest.mark.slow
    def test_full_pipeline(self):
        """æµ‹è¯•å®Œæ•´çš„å¤„ç†æµç¨‹"""
        from processing import DataCleaner, TextChunker, get_embedder
        from storage import QdrantVectorStore
        from models import Paper, Author, SourceType
        from datetime import datetime
        
        # 1. åˆ›å»ºæµ‹è¯•æ•°æ®
        paper = Paper(
            id="test_001",
            title="Attention Is All You Need",
            abstract="The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms.",
            authors=[Author(name="Vaswani et al.")],
            published_date=datetime(2017, 6, 12),
            categories=["cs.CL", "cs.LG"],
            url="https://arxiv.org/abs/1706.03762",
            source=SourceType.ARXIV,
        )
        
        # 2. æ¸…æ´—æ•°æ®
        cleaner = DataCleaner()
        cleaned = cleaner.clean_paper(paper)
        
        assert cleaned["title"] == paper.title
        assert "content" in cleaned
        
        # 3. åˆ†å—
        chunker = TextChunker(chunk_size=200)
        chunks = chunker.chunk(
            cleaned["content"],
            metadata={"paper_id": paper.id, "type": "paper"},
            doc_id=paper.id,
        )
        
        assert len(chunks) >= 1
        
        # 4. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        store = QdrantVectorStore(collection_name="test_integration")
        store.add_chunks(chunks)
        
        # 5. æœç´¢
        results = store.search("attention mechanism", top_k=2)
        
        assert len(results) > 0
        assert "attention" in results[0].content.lower()
        
        # æ¸…ç†
        store.clear()
        print("âœ… Integration test passed!")


# Quick test runner
async def run_quick_test():
    """Run quick tests without pytest"""
    print("ğŸ§ª Running Phase 2 quick tests...\n")
    
    # Test 1: Cleaner
    print("1. Testing DataCleaner...")
    from processing import clean_text, DataCleaner
    
    text = "Check https://example.com for more &amp; info"
    cleaned = clean_text(text)
    assert "https://" not in cleaned
    print(f"   âœ… Cleaned: '{cleaned}'")
    
    # Test 2: Chunker
    print("\n2. Testing TextChunker...")
    from processing import chunk_text
    
    long_text = "This is a sentence. " * 50
    chunks = chunk_text(long_text, chunk_size=200)
    print(f"   âœ… Created {len(chunks)} chunks from {len(long_text)} chars")
    
    # Test 3: Cache
    print("\n3. Testing Cache...")
    from storage import MemoryCache, DiskCache
    
    cache = MemoryCache()
    cache.set("test", {"value": 123})
    result = cache.get("test")
    assert result["value"] == 123
    print(f"   âœ… Cache working: {result}")
    
    # Test 4: Embedder (slow, skip if needed)
    print("\n4. Testing Embedder...")
    try:
        from processing import get_embedder
        embedder = get_embedder()
        embedding = embedder.embed("test text")
        print(f"   âœ… Embedding shape: {embedding.shape}")
    except Exception as e:
        print(f"   âš ï¸ Embedder test skipped: {e}")
    
    # Test 5: Vector Store
    print("\n5. Testing VectorStore...")
    try:
        from storage import QdrantVectorStore
        
        store = QdrantVectorStore(collection_name="quick_test")
        
        store.add(
            documents=["AI is amazing", "Machine learning is cool"],
            metadatas=[{"type": "ai"}, {"type": "ml"}],
        )
        
        results = store.search("artificial intelligence", top_k=1)
        print(f"   âœ… Search result: '{results[0].content}' (score: {results[0].score:.3f})")
        
        store.clear()
    except Exception as e:
        print(f"   âš ï¸ VectorStore test skipped: {e}")
    
    print("\nâœ… All Phase 2 quick tests passed!")


if __name__ == "__main__":
    asyncio.run(run_quick_test())
