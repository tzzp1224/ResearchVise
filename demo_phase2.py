#!/usr/bin/env python
"""
Phase 2 Demo - Processing & Storage Pipeline
æ¼”ç¤ºå®Œæ•´çš„å¤„ç†+å­˜å‚¨æµç¨‹
"""
import os
import sys
from pathlib import Path
from datetime import datetime

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

from rich.console import Console
from rich.panel import Panel
from rich.table import Table

console = Console()


def check_environment():
    """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
    console.print("\n[bold blue]ğŸ”§ ç¯å¢ƒæ£€æŸ¥[/bold blue]\n")
    
    api_key = os.environ.get("SILICONFLOW_API_KEY")
    if api_key:
        console.print(f"  âœ… SILICONFLOW_API_KEY: {api_key[:10]}...{api_key[-4:]}")
        return True
    else:
        console.print("  âš ï¸ SILICONFLOW_API_KEY æœªè®¾ç½®")
        console.print("     è¯·è®¿é—® https://siliconflow.cn/ æ³¨å†Œè·å–å…è´¹ API Key")
        console.print("     ç„¶åè®¾ç½®: set SILICONFLOW_API_KEY=your_key_here\n")
        return False


def demo_cleaner():
    """æ¼”ç¤ºæ•°æ®æ¸…æ´—"""
    console.print("\n[bold green]ğŸ“ 1. æ•°æ®æ¸…æ´— (DataCleaner)[/bold green]\n")
    
    from processing import clean_text
    
    # æ¨¡æ‹Ÿè„æ•°æ®
    dirty_text = """
    Check out this paper: https://arxiv.org/abs/2401.12345
    @researcher mentioned this is &amp; amazing! #DeepLearning
    
    The transformer architecture...  uses    multiple attention heads.
    """
    
    console.print("[dim]åŸå§‹æ–‡æœ¬:[/dim]")
    console.print(Panel(dirty_text, border_style="red"))
    
    # æ¸…æ´—
    cleaned = clean_text(
        dirty_text,
        remove_urls=True,
        remove_mentions=True,
        remove_hashtags=True,
    )
    
    console.print("[dim]æ¸…æ´—å:[/dim]")
    console.print(Panel(cleaned, border_style="green"))


def demo_chunker():
    """æ¼”ç¤ºæ–‡æœ¬åˆ†å—"""
    console.print("\n[bold green]ğŸ“¦ 2. æ–‡æœ¬åˆ†å— (TextChunker)[/bold green]\n")
    
    from processing import TextChunker, ChunkingStrategy
    
    long_text = """
    Transformers have revolutionized natural language processing. 
    The key innovation is the self-attention mechanism, which allows 
    the model to weigh the importance of different parts of the input.
    
    Unlike RNNs, transformers process all positions in parallel.
    This makes them much faster to train on modern hardware.
    The architecture consists of an encoder and decoder, each with 
    multiple layers of attention and feed-forward networks.
    
    BERT, GPT, and T5 are famous transformer-based models.
    They have achieved state-of-the-art results on many NLP tasks.
    """
    
    chunker = TextChunker(
        strategy=ChunkingStrategy.RECURSIVE,
        chunk_size=200,
        chunk_overlap=30,
    )
    
    chunks = chunker.chunk(
        long_text,
        doc_id="demo_doc",
        metadata={"source": "demo"},
    )
    
    table = Table(title="åˆ†å—ç»“æœ", show_header=True)
    table.add_column("Chunk ID", style="cyan")
    table.add_column("Length", style="green")
    table.add_column("Preview", style="white")
    
    for chunk in chunks:
        preview = chunk.content[:50] + "..." if len(chunk.content) > 50 else chunk.content
        table.add_row(
            chunk.id,
            str(len(chunk.content)),
            preview.strip(),
        )
    
    console.print(table)


def demo_embedder(use_api: bool = True):
    """æ¼”ç¤ºå‘é‡åŒ–"""
    console.print("\n[bold green]ğŸ”¢ 3. å‘é‡åŒ– (Embedder)[/bold green]\n")
    
    from processing import get_embedder
    
    if use_api:
        embedder = get_embedder("siliconflow")
        console.print(f"  ä½¿ç”¨: SiliconFlow BGE-M3 (ç»´åº¦: {embedder.dimension})")
    else:
        embedder = get_embedder("sentence_transformers")
        console.print(f"  ä½¿ç”¨: SentenceTransformers (ç»´åº¦: {embedder.dimension})")
    
    texts = [
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œ",
        "Transformers are attention-based models",
    ]
    
    console.print("\n  [dim]è¾“å…¥æ–‡æœ¬:[/dim]")
    for i, text in enumerate(texts):
        console.print(f"    {i+1}. {text}")
    
    console.print("\n  [dim]è®¡ç®—å‘é‡...[/dim]")
    embeddings = embedder.embed(texts)
    
    console.print(f"\n  âœ… ç”Ÿæˆå‘é‡: shape = {embeddings.shape}")
    console.print(f"     æ¯ä¸ªæ–‡æœ¬ â†’ {embeddings.shape[1]} ç»´å‘é‡")
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    import numpy as np
    def cosine_similarity(a, b):
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    console.print("\n  [dim]æ–‡æœ¬ç›¸ä¼¼åº¦:[/dim]")
    for i in range(len(texts)):
        for j in range(i+1, len(texts)):
            sim = cosine_similarity(embeddings[i], embeddings[j])
            console.print(f"    [{i+1}] vs [{j+1}]: {sim:.4f}")
    
    return embedder


def demo_vector_store(embedder):
    """æ¼”ç¤ºå‘é‡å­˜å‚¨"""
    console.print("\n[bold green]ğŸ’¾ 4. å‘é‡å­˜å‚¨ (Qdrant)[/bold green]\n")
    
    from storage import QdrantVectorStore
    from processing import chunk_document
    
    # æ¨¡æ‹Ÿè®ºæ–‡æ•°æ®
    paper_abstract = """
    We introduce the Transformer, a new architecture for sequence transduction.
    Unlike recurrent models, the Transformer relies entirely on attention mechanisms.
    Our experiments show the model achieves state-of-the-art results on translation tasks.
    The Transformer is more parallelizable and requires significantly less time to train.
    """
    
    # åˆ†å—
    chunks = chunk_document(
        content=paper_abstract,
        doc_id="paper_001",
        doc_type="paper",
        metadata={"title": "Attention Is All You Need", "year": 2017},
        chunk_size=150,
    )
    
    # è®¡ç®—å‘é‡
    texts = [c.content for c in chunks]
    embeddings = embedder.embed(texts)
    
    # å­˜å‚¨ (ä½¿ç”¨ Qdrant)
    store = QdrantVectorStore(
        collection_name="demo_papers",
        persist_directory=None,  # å†…å­˜æ¨¡å¼
        dimension=embedder.dimension,
    )
    
    # æ·»åŠ åˆ°å­˜å‚¨
    store.add_with_embeddings(
        documents=texts,
        embeddings=embeddings.tolist(),
        metadatas=[c.metadata for c in chunks],
        ids=[c.id for c in chunks],
    )
    
    console.print(f"  âœ… å­˜å‚¨ {store.count()} ä¸ªæ–‡æ¡£å—")
    
    # æ¼”ç¤ºå…ƒæ•°æ®è¿‡æ»¤ (Qdrant ç‰¹æ€§)
    console.print("\n  [dim]Qdrant æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤:[/dim]")
    console.print("    - filter={\"year\": {\"$gte\": 2015}}")
    console.print("    - filter={\"topic\": \"LLM\"}")
    
    # æœç´¢
    query = "What is the main innovation?"
    console.print(f"\n  [dim]æœç´¢: '{query}'[/dim]")
    
    query_embedding = embedder.embed(query)
    results = store.search_with_embedding(
        query_embedding=query_embedding[0].tolist(),
        top_k=2,
    )
    
    console.print("\n  [bold]æœç´¢ç»“æœ:[/bold]")
    for i, result in enumerate(results):
        console.print(f"    {i+1}. [Score: {result.score:.4f}]")
        console.print(f"       {result.content[:80]}...")
    
    # æ¸…ç†
    store.clear()


def demo_cache():
    """æ¼”ç¤ºç¼“å­˜"""
    console.print("\n[bold green]ğŸ’¨ 5. ç¼“å­˜ (Cache)[/bold green]\n")
    
    from storage import MemoryCache
    
    # å†…å­˜ç¼“å­˜
    cache = MemoryCache(ttl=60)
    
    cache.set("query_result", {
        "query": "transformer attention",
        "results": ["paper1", "paper2"],
        "timestamp": datetime.now().isoformat(),
    })
    
    result = cache.get("query_result")
    console.print(f"  âœ… ç¼“å­˜è¯»å†™æ­£å¸¸: {result}")


def main():
    """ä¸»å‡½æ•°"""
    console.print(Panel.fit(
        "[bold blue]Phase 2 Demo: Processing & Storage Pipeline[/bold blue]\n"
        "æ¼”ç¤ºæ•°æ®æ¸…æ´—ã€åˆ†å—ã€å‘é‡åŒ–ã€å­˜å‚¨çš„å®Œæ•´æµç¨‹",
        border_style="blue",
    ))
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    from dotenv import load_dotenv
    load_dotenv(Path(__file__).parent / "config" / ".env")
    
    # 1. æ£€æŸ¥ç¯å¢ƒ
    has_api_key = check_environment()
    
    # 2. æ¼”ç¤ºæ¸…æ´—
    demo_cleaner()
    
    # 3. æ¼”ç¤ºåˆ†å—
    demo_chunker()
    
    # 4. æ¼”ç¤ºç¼“å­˜ (ä¸éœ€è¦ API)
    demo_cache()
    
    if has_api_key:
        # 5. æ¼”ç¤ºå‘é‡åŒ– (éœ€è¦ API)
        embedder = demo_embedder(use_api=True)
        
        # 6. æ¼”ç¤ºå‘é‡å­˜å‚¨ (éœ€è¦ API)
        demo_vector_store(embedder)
        
        console.print("\n" + "="*50)
        console.print("[bold green]âœ… Phase 2 å®Œæ•´æ¼”ç¤ºå®Œæˆï¼[/bold green]\n")
    else:
        console.print("\n" + "="*50)
        console.print("[yellow]âš ï¸ éƒ¨åˆ†æ¼”ç¤ºè·³è¿‡ (éœ€è¦ API Key)[/yellow]")
        console.print("è®¾ç½® SILICONFLOW_API_KEY åå¯ä½“éªŒå®Œæ•´åŠŸèƒ½\n")


if __name__ == "__main__":
    main()
